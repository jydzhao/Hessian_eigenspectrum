{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6574c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "from IPython.display import display, Latex\n",
    "from torch.autograd.functional import jacobian\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f86ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5edb86f0",
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "# Create model of linear NN with L hidden layers\n",
    "# input dim = d, hidden dim = m, output dim = k\n",
    "class Linear_NN(nn.Module):\n",
    "    def __init__(self,d,m,k,L):\n",
    "        \"\"\"\n",
    "            d: input dimension\n",
    "            m: hidden layer dimension \n",
    "            k: output dimension\n",
    "            L: number of hidden layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.L = L\n",
    "        self.lin_out = nn.Linear(m, k, bias=False)\n",
    "        self.lin_in = nn.Linear(d, m, bias=False)\n",
    "        \n",
    "        self.lin_hidden = nn.ModuleList([nn.Linear(m, m, bias=False) for i in range(self.L)])\n",
    "        \n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = self.lin_in(xb)\n",
    "        \n",
    "        for i in range(self.L):\n",
    "            xb = self.lin_hidden[i](xb)\n",
    "            \n",
    "        xb = self.lin_out(xb)\n",
    "        \n",
    "        return xb\n",
    "    \n",
    "    def init_weights(self, init_type):\n",
    "        if init_type == 'kaiming_normal':\n",
    "            torch.nn.init.kaiming_normal_(self.lin_in.weight, nonlinearity='linear')\n",
    "            torch.nn.init.kaiming_normal_(self.lin_out.weight, nonlinearity='linear')\n",
    "            for i in range(self.L):\n",
    "                torch.nn.init.kaiming_normal_(self.lin_hidden[i].weight, nonlinearity='linear')\n",
    "        elif init_type == 'kaiming_uniform':\n",
    "            torch.nn.init.kaiming_uniform_(self.lin_in.weight, nonlinearity='linear')\n",
    "            torch.nn.init.kaiming_uniform_(self.lin_out.weight, nonlinearity='linear')\n",
    "            for i in range(self.L):\n",
    "                torch.nn.init.kaiming_uniform_(self.lin_hidden[i].weight, nonlinearity='linear')\n",
    "        elif init_type == 'xavier_normal':\n",
    "            torch.nn.init.xavier_normal_(self.lin_in.weight)\n",
    "            torch.nn.init.xavier_normal_(self.lin_out.weight)\n",
    "            for i in range(self.L):\n",
    "                torch.nn.init.xavier_normal_(self.lin_hidden[i].weight)\n",
    "        elif init_type == 'xavier_uniform':\n",
    "            torch.nn.init.xavier_uniform_(self.lin_in.weight, nonlinearity='linear')\n",
    "            torch.nn.init.xavier_uniform_(self.lin_out.weight, nonlinearity='linear')\n",
    "            for i in range(self.L):\n",
    "                torch.nn.init.xavier_uniform_(self.lin_hidden[i].weight, nonlinearity='linear')\n",
    "        else:\n",
    "            print('Unknown initialization. Using Kaiming normal initialization')\n",
    "            torch.nn.init.kaiming_normal_(self.lin1.weight, nonlinearity='linear')\n",
    "            torch.nn.init.kaiming_normal_(self.lin2.weight, nonlinearity='linear')\n",
    "            for i in range(self.L):\n",
    "                torch.nn.init.kaiming_normal_(self.lin_hidden[i].weight, nonlinearity='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc3d4715",
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "# Create model of linear NN with L hidden layers\n",
    "# input dim = d, hidden dim = m, output dim = k\n",
    "class ReLU_NN(nn.Module):\n",
    "    def __init__(self,d,m,k,L):\n",
    "        \"\"\"\n",
    "            d: input dimension\n",
    "            m: hidden layer dimension \n",
    "            k: output dimension\n",
    "            L: number of hidden layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.L = L\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin_out = nn.Linear(m, k, bias=False)\n",
    "        self.lin_in = nn.Linear(d, m, bias=False)\n",
    "        \n",
    "        self.lin_hidden = nn.ModuleList([nn.Linear(m, m, bias=False) for i in range(self.L)])\n",
    "        \n",
    "        self.sequential = nn.Sequential(self.lin_in)\n",
    "        \n",
    "        for i in range(self.L):\n",
    "            self.sequential.append(self.relu)\n",
    "            self.sequential.append(self.lin_hidden[i])\n",
    "        \n",
    "        self.sequential.append(self.relu)\n",
    "        self.sequential.append(self.lin_out)                \n",
    "        \n",
    "        \n",
    "    def forward(self, xb):\n",
    "#         xb = self.lin_in(xb)\n",
    "                \n",
    "#         for i in range(self.L):\n",
    "#             xb = self.relu(xb)\n",
    "            \n",
    "#             xb = self.lin_hidden[i](xb)\n",
    "        \n",
    "#         xb = self.relu(xb)\n",
    "                        \n",
    "#         xb = self.lin_out(xb)\n",
    "        xb = self.sequential(xb)\n",
    "        \n",
    "        return xb\n",
    "    \n",
    "    def init_weights(self, init_type):\n",
    "        if init_type == 'kaiming_normal':\n",
    "            torch.nn.init.kaiming_normal_(self.lin_in.weight, nonlinearity='linear')\n",
    "            torch.nn.init.kaiming_normal_(self.lin_out.weight, nonlinearity='linear')\n",
    "            for i in range(self.L):\n",
    "                torch.nn.init.kaiming_normal_(self.lin_hidden[i].weight, nonlinearity='linear')\n",
    "        elif init_type == 'kaiming_uniform':\n",
    "            torch.nn.init.kaiming_uniform_(self.lin_in.weight, nonlinearity='linear')\n",
    "            torch.nn.init.kaiming_uniform_(self.lin_out.weight, nonlinearity='linear')\n",
    "            for i in range(self.L):\n",
    "                torch.nn.init.kaiming_uniform_(self.lin_hidden[i].weight, nonlinearity='linear')\n",
    "        elif init_type == 'xavier_normal':\n",
    "            torch.nn.init.xavier_normal_(self.lin_in.weight)\n",
    "            torch.nn.init.xavier_normal_(self.lin_out.weight)\n",
    "            for i in range(self.L):\n",
    "                torch.nn.init.xavier_normal_(self.lin_hidden[i].weight)\n",
    "        elif init_type == 'xavier_uniform':\n",
    "            torch.nn.init.xavier_uniform_(self.lin_in.weight, nonlinearity='linear')\n",
    "            torch.nn.init.xavier_uniform_(self.lin_out.weight, nonlinearity='linear')\n",
    "            for i in range(self.L):\n",
    "                torch.nn.init.xavier_uniform_(self.lin_hidden[i].weight, nonlinearity='linear')\n",
    "        else:\n",
    "            print('Unknown initialization. Using Kaiming normal initialization')\n",
    "            torch.nn.init.kaiming_normal_(self.lin1.weight, nonlinearity='linear')\n",
    "            torch.nn.init.kaiming_normal_(self.lin2.weight, nonlinearity='linear')\n",
    "            for i in range(self.L):\n",
    "                torch.nn.init.kaiming_normal_(self.lin_hidden[i].weight, nonlinearity='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2db0a14a",
   "metadata": {
    "code_folding": [
     32,
     43,
     65,
     76
    ]
   },
   "outputs": [],
   "source": [
    "# Author: ludwigwinkler\n",
    "# Source: https://discuss.pytorch.org/t/get-gradient-and-jacobian-wrt-the-parameters/98240/6\n",
    "\n",
    "import future, sys, os, datetime, argparse\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [10, 10]\n",
    "\n",
    "import torch, torch.nn\n",
    "from torch import nn\n",
    "from torch.nn import Sequential, Module, Parameter\n",
    "from torch.nn import Linear, Tanh, ReLU\n",
    "import torch.nn.functional as F\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "FloatTensor = torch.FloatTensor\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "sys.path.append(\"../../..\")  # Up to -> KFAC -> Optimization -> PHD\n",
    "\n",
    "import copy\n",
    "\n",
    "cwd = os.path.abspath(os.getcwd())\n",
    "os.chdir(cwd)\n",
    "\n",
    "# from Optimization.BayesianGradients.src.DeterministicLayers import GradBatch_Linear as Linear\n",
    "\n",
    "\n",
    "def _del_nested_attr(obj: nn.Module, names: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Deletes the attribute specified by the given list of names.\n",
    "    For example, to delete the attribute obj.conv.weight,\n",
    "    use _del_nested_attr(obj, ['conv', 'weight'])\n",
    "    \"\"\"\n",
    "    if len(names) == 1:\n",
    "        delattr(obj, names[0])\n",
    "    else:\n",
    "        _del_nested_attr(getattr(obj, names[0]), names[1:])\n",
    "\n",
    "def extract_weights(mod: nn.Module) -> Tuple[Tuple[Tensor, ...], List[str]]:\n",
    "    \"\"\"\n",
    "    This function removes all the Parameters from the model and\n",
    "    return them as a tuple as well as their original attribute names.\n",
    "    The weights must be re-loaded with `load_weights` before the model\n",
    "    can be used again.\n",
    "    Note that this function modifies the model in place and after this\n",
    "    call, mod.parameters() will be empty.\n",
    "    \"\"\"\n",
    "    orig_params = tuple(mod.parameters())\n",
    "    # Remove all the parameters in the model\n",
    "    names = []\n",
    "    for name, p in list(mod.named_parameters()):\n",
    "        _del_nested_attr(mod, name.split(\".\"))\n",
    "        names.append(name)\n",
    "\n",
    "    '''\n",
    "        Make params regular Tensors instead of nn.Parameter\n",
    "    '''\n",
    "    params = tuple(p.detach().requires_grad_() for p in orig_params)\n",
    "    return params, names\n",
    "\n",
    "def _set_nested_attr(obj: Module, names: List[str], value: Tensor) -> None:\n",
    "    \"\"\"\n",
    "    Set the attribute specified by the given list of names to value.\n",
    "    For example, to set the attribute obj.conv.weight,\n",
    "    use _del_nested_attr(obj, ['conv', 'weight'], value)\n",
    "    \"\"\"\n",
    "    if len(names) == 1:\n",
    "        setattr(obj, names[0], value)\n",
    "    else:\n",
    "        _set_nested_attr(getattr(obj, names[0]), names[1:], value)\n",
    "\n",
    "def load_weights(mod: Module, names: List[str], params: Tuple[Tensor, ...]) -> None:\n",
    "    \"\"\"\n",
    "    Reload a set of weights so that `mod` can be used again to perform a forward pass.\n",
    "    Note that the `params` are regular Tensors (that can have history) and so are left\n",
    "    as Tensors. This means that mod.parameters() will still be empty after this call.\n",
    "    \"\"\"\n",
    "    for name, p in zip(names, params):\n",
    "        _set_nested_attr(mod, name.split(\".\"), p)\n",
    "\n",
    "def compute_jacobian(model, x):\n",
    "    '''\n",
    "\n",
    "    @param model: model with vector output (not scalar output!) the parameters of which we want to compute the Jacobian for\n",
    "    @param x: input since any gradients requires some input\n",
    "    @return: either store jac directly in parameters or store them differently\n",
    "\n",
    "    we'll be working on a copy of the model because we don't want to interfere with the optimizers and other functionality\n",
    "    '''\n",
    "\n",
    "    jac_model = copy.deepcopy(model) # because we're messing around with parameters (deleting, reinstating etc)\n",
    "    all_params, all_names = extract_weights(jac_model) # \"deparameterize weights\"\n",
    "    load_weights(jac_model, all_names, all_params) # reinstate all weights as plain tensors\n",
    "\n",
    "    def param_as_input_func(model, x, param):\n",
    "        load_weights(model, [name], [param]) # name is from the outer scope\n",
    "        out = model(x)\n",
    "        return out\n",
    "\n",
    "    jacobian=np.zeros(1)    \n",
    "    for i, (name, param) in enumerate(zip(all_names, all_params)):\n",
    "        jac = torch.autograd.functional.jacobian(lambda param: param_as_input_func(jac_model, x, param), param,\n",
    "                             strict=True if i==0 else False, vectorize=False if i==0 else True)\n",
    "        print(jac.shape)\n",
    "        j = torch.reshape(jac,(n,k,jac.shape[-1]*jac.shape[-2]))\n",
    "        print(j.shape)\n",
    "        if i==0:\n",
    "            jacobian = j\n",
    "        else:\n",
    "            jacobian = torch.cat([jacobian,j],dim=2)\n",
    "#     print(jacobian.shape)        \n",
    "\n",
    "    del jac_model # cleaning up\n",
    "    \n",
    "    return jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e85dfb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num parameters:  [60]\n"
     ]
    }
   ],
   "source": [
    "d = 2 # input dimension\n",
    "m1 = [5] # hidden layer dimension\n",
    "k = 10 # output dimension\n",
    "L = [0] # number of hidden layers of dim \"m\"\n",
    "    \n",
    "m_L_config = [] # keep track of the network configuration\n",
    "num_param = [] # count the number of parameters in the model\n",
    "Linear_Networks = [] # list of NN with different configurations\n",
    "ReLU_Networks = [] # list of ReLU NN with different configurations\n",
    "\n",
    "\n",
    "# initiate linear networks of given depth L[l] with m1 hidden units each\n",
    "for m in m1:\n",
    "    for l in L:\n",
    "        m_L_config.append((m,l))\n",
    "        Linear_Networks.append(Linear_NN(d,m,k,l))\n",
    "        ReLU_Networks.append(ReLU_NN(d,m,k,l))\n",
    "        num_param.append(sum(p.numel() for p in Linear_NN(d,m,k,l).parameters()))\n",
    "        \n",
    "print('num parameters: ', num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5c1e12b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "torch.manual_seed(314159)\n",
    "x = torch.randn(n,d).requires_grad_()\n",
    "cov_xx = x.detach().T @ x.detach() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a6e9f553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bdc5374a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8384, 1.1571])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.eigvalsh(cov_xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f4566adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10, 10, 5])\n",
      "torch.Size([100, 10, 50])\n",
      "torch.Size([100, 10, 5, 2])\n",
      "torch.Size([100, 10, 10])\n",
      "Jacobian shape torch.Size([100, 10, 60])\n"
     ]
    }
   ],
   "source": [
    "Linear_Networks[0].init_weights('kaiming_normal')\n",
    "\n",
    "jacob = compute_jacobian(Linear_Networks[0],x)\n",
    "print('Jacobian shape', jacob.shape)\n",
    "\n",
    "V_kaiming = Linear_Networks[0].lin_in.weight.detach()\n",
    "W_kaiming = Linear_Networks[0].lin_out.weight.detach()\n",
    "\n",
    "# calculate the outer Hessian product according to the expression derived by Sidak\n",
    "H_o_tilde_lin = torch.kron( W_kaiming @ W_kaiming.T, cov_xx ) + \\\n",
    "                torch.kron( torch.eye(k), V_kaiming.T @ V_kaiming @ cov_xx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "09678372",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# W_kaiming @ V_kaiming @ x.T.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f94c97d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Linear_Networks[0].forward(x.detach()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4c7bcca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 20])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_o_tilde_lin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fa21ed1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.matrix_rank(H_o_tilde_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d600f6df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3715, 1.3715, 1.3715, 1.3715, 1.3715, 1.6049, 1.8692, 2.1158, 3.5034,\n",
       "        4.6688, 5.2732, 5.2732, 5.2732, 5.2732, 5.2732, 5.5131, 5.7864, 6.0432,\n",
       "        7.5188, 8.7981])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.linalg.eigvalsh(H_o_tilde_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "685b2f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [jacob[i,:,:] @ jacob[i,:,:].T for i in range(n)]\n",
    "\n",
    "jac_jac_T = sum(arr)/n\n",
    "torch.linalg.matrix_rank(jac_jac_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "58475383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jac_jac_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1a6ab9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.6447,  6.6447,  6.6447,  6.6447,  6.6447,  7.1180,  7.6556,  8.1590,\n",
       "        11.0222, 13.4668])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.eigvalsh(jac_jac_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b172a955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66500253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
